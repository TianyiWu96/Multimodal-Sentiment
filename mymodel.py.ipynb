{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(1337)\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch, torch.nn as nn, math, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "import torch.utils.data\n",
    "Batch_size = 10\n",
    "Epochs = 200\n",
    "validation_split = 0.2\n",
    "shuffle = True\n",
    "\n",
    "def data_load(model, batch_size = 10, valid_size = 0.2):\n",
    "    # train, validation, test data \n",
    "    # question 1, if the train data and test data are from the same data distribution, will this be problem that the model overfit to the dataset\n",
    "    \n",
    "    with open('./input/'+model+'.pickle', 'rb') as handle:\n",
    "                (train_x, train_y, test_x, test_y, maxlen, train_len, test_len) = pickle.load(handle)\n",
    "    train_len = np.array(train_len)\n",
    "    test_len = np.array(test_len)\n",
    "    \n",
    "    train_x = torch.from_numpy(train_x).double()\n",
    "    train_y = torch.from_numpy(train_y).double()\n",
    "    test_x = torch.from_numpy(test_x).double()\n",
    "    test_y = torch.from_numpy(test_y).double()\n",
    "    train_len = torch.from_numpy(train_len)\n",
    "    test_len = torch.from_numpy(test_len)\n",
    "    \n",
    "    num_train = len(train_x) \n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    random_seed = 2018\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(train_x, train_y, train_len)\n",
    "    test = torch.utils.data.TensorDataset(test_x, test_y, test_len)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=train_sampler, num_workers=2)\n",
    "    valid_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=valid_sampler, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size= batch_size, shuffle= True, num_workers=2)\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc1 = nn.Linear(600,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "       \n",
    "    def forward(self, inputs, sequence_len): # input is 10 * 63 * 100\n",
    "        # training details:\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, sequence_len, batch_first=True)\n",
    "        hidden = (\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True),\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True))\n",
    "        #print(pack)\n",
    "        out, hidden = self.lstm(pack, hidden)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #print(unpacked.size())\n",
    "        output = self.dropout2(self.tanh(unpacked)) # apply drop out\n",
    "        inter1 = self.dropout2(self.relu(self.fc1(output))) # 100\n",
    "        output = self.fc2(inter1) # 2\n",
    "        #print(output)\n",
    "        return output, inter1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4248, -1.2822, -0.6673],\n",
      "        [-0.7594,  0.4253, -0.3516]])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(2, 3))\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# training unimodel, use the index to trace the loss function.\n",
    "import os\n",
    "epochs = 30\n",
    "models = ['text', 'audio', 'video']\n",
    "input_size = {'text': 100, 'audio': 73, 'video':100}\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "use_gpu = False\n",
    "print(use_gpu)\n",
    "unimodal_activations = {}\n",
    "save_dir = '/home/linux/ieng6/cs291g/cs291gbn/Multimodal-Sentiment/outputs/'\n",
    "\n",
    "def sorted_sequence(input_x,sequences,y):\n",
    "    # return the sorted sequence based on input_x\n",
    "    diction = {}\n",
    "    for i in range(int(sequences.size(0))):\n",
    "        diction[i] = sequences[i]\n",
    "    new_sequence = []\n",
    "    new_x = torch.zeros(input_x.size(0),input_x.size(1),input_x.size(2))\n",
    "    new_y = torch.zeros(y.size(0),y.size(1))\n",
    "    count = 0\n",
    "    for i, value in sorted(diction.items(), key=lambda x:x[1], reverse = True):\n",
    "        new_sequence.append(int(value))\n",
    "        new_x[count] = input_x[i]\n",
    "        new_y[count] = y[i]\n",
    "        count +=1\n",
    "    return new_x,new_sequence,new_y\n",
    "\n",
    "def cast_y(max_len, y):\n",
    "    new_y = torch.zeros(y.size(0), max_len)\n",
    "    for i in range(y.size(0)):\n",
    "        new_y[i] = y[i][:max_len]\n",
    "    return new_y\n",
    "\n",
    "def save_model(net, optim, epoch, ckpt_fname):  \n",
    "        state_dict = net.state_dict()                                                                                                                                                                         \n",
    "        for key in state_dict.keys():                                                                                                                                                                                \n",
    "            state_dict[key] = state_dict[key].cpu()                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        torch.save({                                                                                                                                                                                                 \n",
    "            'epoch': epoch,                                                                                                                                                                                     \n",
    "            'state_dict': state_dict,                                                                                                                                                                                \n",
    "            'optimizer': optim},                                                                                                                                                                                     \n",
    "            ckpt_fname)\n",
    "        \n",
    "def one_hot(train_y):\n",
    "    maxlabel = 1\n",
    "    new_y = torch.zeros((train_y.size(0),train_y.size(1), maxlabel+1))\n",
    "    for i in range(int(train_y.size(0))):\n",
    "        for j in range(int(train_y.size(1))):\n",
    "            new_y[i,j,int(train_y[i,j])] = 1 \n",
    "    return new_y\n",
    "def generate_weight(sequence_length, y):\n",
    "    mask = torch.zeros((y.size(0),y.size(1)))\n",
    "    for i in range(y.size(0)):\n",
    "        for j in range(sequence_length[i]):\n",
    "            mask[i,j] = 1.0\n",
    "    return mask\n",
    "            \n",
    "train_loss = {}\n",
    "train_acc = {}\n",
    "test_loss = {}\n",
    "test_acc = {}\n",
    "valid_acc = {}\n",
    "valid_loss = {}\n",
    "\n",
    "train_loss['text'] = []\n",
    "train_loss['audio'] = []\n",
    "train_loss['video'] = []\n",
    "train_acc['text'] = []\n",
    "train_acc['audio'] = []\n",
    "train_acc['video'] = []\n",
    "\n",
    "test_loss['text'] = []\n",
    "test_loss['audio'] = []\n",
    "test_loss['video'] = []\n",
    "test_acc['text'] = []\n",
    "test_acc['audio'] = []\n",
    "test_acc['video'] = []\n",
    "\n",
    "valid_acc['text'] = []\n",
    "valid_acc['audio'] = []\n",
    "valid_acc['video'] = []\n",
    "valid_loss['text'] = []\n",
    "valid_loss['audio'] = []\n",
    "valid_loss['audio'] = []\n",
    "\n",
    "unimodal_results = {}\n",
    "criterion = nn.CrossEntropyLoss(size_average = False)\n",
    "\n",
    "def train():\n",
    "    for mode in models:\n",
    "        model = Unimodel(input_size[mode])\n",
    "        train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "        if(use_gpu):\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adagrad(params = model.parameters(), lr = 0.01)\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        print('begin training for unimodel ' + mode)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for e, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                if use_gpu:\n",
    "                    input_x = Variable(input_x.cuda(), requires_grad=True)\n",
    "                else:\n",
    "                    input_x = Variable(input_x, requires_grad=True)\n",
    "                y = cast_y(sequence_len[0], y).long()\n",
    "                if use_gpu:\n",
    "                    y = Variable(y.cuda())\n",
    "                predict_y, inter1 = model(input_x, sequence_len)\n",
    "                #train_mask = generate_weight(sequence_len, y)\n",
    "                #print(train_mask.shape) # you can't use that weight because its for the whole class.\n",
    "                # instead, \n",
    "                loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                correct += (predicted.data == y.view(-1).data).sum().int().data[0]\n",
    "                print(correct)\n",
    "                break\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.data[0]\n",
    "                total += sum(sequence_len)\n",
    "            train_loss[mode].append(running_loss)\n",
    "            train_acc[mode].append(1.* correct/total)\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            for e, data in enumerate(valid_loader):\n",
    "                    input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                    input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                    if use_gpu:\n",
    "                        input_x = Variable(input_x.cuda(), requires_grad=False)\n",
    "                    else:\n",
    "                        input_x = Variable(input_x, requires_grad=False)\n",
    "                    y = cast_y(sequence_len[0], y).long()\n",
    "                    if use_gpu:\n",
    "                        y = Variable(y.cuda())\n",
    "                    predict_y, inter2 = model(input_x, sequence_len) \n",
    "                    loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                    running_loss += loss\n",
    "                    _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                    correct += (predicted.data == y.view(-1).data).sum().int().data[0]\n",
    "                    total += sum(sequence_len)\n",
    "                    if(epoch %1 == 0):\n",
    "                        print(\"epoch %d train acc %g valid acc % f\" %(epoch, train_acc[mode][-1], 1.* correct/total))\n",
    "                        valid_loss[mode].append(running_loss)\n",
    "                        valid_acc[mode].append(1.* correct/total)\n",
    "                        running_loss = 0\n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "            for e, data in enumerate(test_loader):\n",
    "                    input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                    input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                    if use_gpu:\n",
    "                        input_x = Variable(input_x.cuda(), requires_grad=False)\n",
    "                    else:\n",
    "                        input_x = Variable(input_x, requires_grad=False)\n",
    "                    y = cast_y(sequence_len[0], y).long()\n",
    "                    if use_gpu:\n",
    "                        y = Variable(y.cuda())\n",
    "                    predict_y, inter2 = model(input_x, sequence_len) \n",
    "                    loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                    _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                    correct += (predicted.data == y.view(-1).data).sum().int().data[0]\n",
    "                    running_loss += loss.data[0]\n",
    "                    total += sum(sequence_len)\n",
    "                    if epoch % 5 == 0:\n",
    "                        print(\"test loss %f acc %g\" %(running_loss, 1.* correct.item()/total))\n",
    "                    test_loss[mode].append(running_loss)\n",
    "                    test_acc[mode].append(1.* correct.item()/total)\n",
    "                    running_loss = 0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "            if(epoch > 10 and valid_acc[mode][-1] < valid_acc[mode][-2] and valid_acc[mode][-2] < valid_acc[mode][-3]):\n",
    "                print('stop early at %d' % epoch)\n",
    "                break\n",
    "        filename = 'result/train_' + mode+  '_epoch_%d.dat' % (epoch +1)\n",
    "        with open(filename,'w'):\n",
    "            save_model(model, optimizer, epoch, filename)  \n",
    "        unimodal_activations[mode +'_train'] = inter1\n",
    "    with open('result/epoch%d'%(epoch +1) + 'unimodal.pickle', 'wb') as handle:\n",
    "        pickle.dump(unimodal_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training for unimodel text\n",
      "epoch 0 train acc 1 valid acc  2.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6c9e7ab4c92c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-e4ea8000750c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %d train acc %g valid acc % f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                         \u001b[0mvalid_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_loss' is not defined"
     ]
    }
   ],
   "source": [
    "#train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.4)\n",
    "        self.fc1 = nn.Linear(hidden_size,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "        self.output = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        hidden_vect = (\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)),\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        output, hidden_vect = self.lstm(Variable(inputs), hidden_vect)\n",
    "        output = self.dropout2(self.tanh(output)) # apply drop out\n",
    "        output = self.dropout2(self.tanh(self.fc1(output))) # 100\n",
    "        output = self.softmax(self.fc2(output)) # 2\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 39, 100])\n",
      "torch.Size([10, 30, 100])\n",
      "torch.Size([10, 39, 100])\n"
     ]
    }
   ],
   "source": [
    "with open('result/unimodal.pickle', 'rb') as handle:\n",
    "    unimodal_activations = pickle.load(handle)\n",
    "print(unimodal_activations['text_train'].size())\n",
    "print(unimodal_activations['video_train'].size())\n",
    "print(unimodal_activations['audio_train'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
