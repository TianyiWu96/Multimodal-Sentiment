{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(1337)\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch, torch.nn as nn, math, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "import torch.utils.data\n",
    "Batch_size = 10\n",
    "Epochs = 200\n",
    "validation_split = 0.2\n",
    "shuffle = True\n",
    "\n",
    "def data_load(model, batch_size = 10, valid_size = 0.2):\n",
    "    # train, validation, test data \n",
    "    # question 1, if the train data and test data are from the same data distribution, will this be problem that the model overfit to the dataset\n",
    "    \n",
    "    with open('./input/'+model+'.pickle', 'rb') as handle:\n",
    "                (train_x, train_y, test_x, test_y, maxlen, train_len, test_len) = pickle.load(handle)\n",
    "    train_len = np.array(train_len)\n",
    "    test_len = np.array(test_len)\n",
    "    \n",
    "    train_x = torch.from_numpy(train_x).double()\n",
    "    train_y = torch.from_numpy(train_y).double()\n",
    "    test_x = torch.from_numpy(test_x).double()\n",
    "    test_y = torch.from_numpy(test_y).double()\n",
    "    train_len = torch.from_numpy(train_len)\n",
    "    test_len = torch.from_numpy(test_len)\n",
    "    \n",
    "    num_train = len(train_x) \n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    random_seed = 2018\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(train_x, train_y, train_len)\n",
    "    test = torch.utils.data.TensorDataset(test_x, test_y, test_len)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=train_sampler, num_workers=2)\n",
    "    valid_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=valid_sampler, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size= batch_size, shuffle= True, num_workers=2)\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc1 = nn.Linear(600,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "       \n",
    "    def forward(self, inputs, sequence_len): # input is 10 * 63 * 100\n",
    "        # training details:\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, sequence_len, batch_first=True)\n",
    "        hidden = (\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True),\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True))\n",
    "        #print(pack)\n",
    "        out, hidden = self.lstm(pack, hidden)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #print(unpacked.size())\n",
    "        output = self.dropout2(self.tanh(unpacked)) # apply drop out\n",
    "        inter1 = self.dropout2(self.tanh(self.fc1(output))) # 100\n",
    "        output = self.fc2(inter1) # 2\n",
    "        #print(output)\n",
    "        return output, inter1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# training unimodel, use the index to trace the loss function.\n",
    "import os\n",
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "models = ['text', 'audio', 'video']\n",
    "input_size = {'text': 100, 'audio': 73, 'video':100}\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "use_gpu = False\n",
    "print(use_gpu)\n",
    "unimodal_activations = {}\n",
    "save_dir = 'cs291gbn@cs291gbn-30904:~/Multimodal-Sentiment/result/'\n",
    "\n",
    "def sorted_sequence(input_x,sequences,y):\n",
    "    # return the sorted sequence based on input_x\n",
    "    diction = {}\n",
    "    for i in range(int(sequences.size(0))):\n",
    "        diction[i] = sequences[i]\n",
    "    new_sequence = []\n",
    "    new_x = torch.zeros(input_x.size(0),input_x.size(1),input_x.size(2))\n",
    "    new_y = torch.zeros(y.size(0),y.size(1))\n",
    "    count = 0\n",
    "    for i, value in sorted(diction.items(), key=lambda x:x[1], reverse = True):\n",
    "        new_sequence.append(int(value))\n",
    "        new_x[count] = input_x[i]\n",
    "        new_y[count] = y[i]\n",
    "        count +=1\n",
    "    return new_x,new_sequence,new_y\n",
    "\n",
    "def cast_y(max_len, y):\n",
    "    new_y = torch.zeros(y.size(0), max_len)\n",
    "    for i in range(y.size(0)):\n",
    "        new_y[i] = y[i][:max_len]\n",
    "    return new_y\n",
    "\n",
    "def save_model(net, optim, ckpt_fname):                                                                                                                                                             \n",
    "        state_dict = net.state_dict()                                                                                                                                                                         \n",
    "        for key in state_dict.keys():                                                                                                                                                                                \n",
    "            state_dict[key] = state_dict[key].cpu()                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        torch.save({                                                                                                                                                                                                 \n",
    "            'epoch': epoch,                                                                                                                                                                                     \n",
    "            'state_dict': state_dict,                                                                                                                                                                                \n",
    "            'optimizer': optim},                                                                                                                                                                                     \n",
    "            ckpt_fname)\n",
    "        \n",
    "def one_hot(train_y):\n",
    "    maxlabel = 1\n",
    "    new_y = torch.zeros((train_y.size(0),train_y.size(1), maxlabel+1))\n",
    "    for i in range(int(train_y.size(0))):\n",
    "        for j in range(int(train_y.size(1))):\n",
    "            new_y[i,j,int(train_y[i,j])] = 1 \n",
    "    return new_y\n",
    "train_loss = {}\n",
    "train_acc = {}\n",
    "test_loss = {}\n",
    "test_acc = {}\n",
    "\n",
    "train_loss['text'] = []\n",
    "train_loss['audio'] = []\n",
    "train_loss['video'] = []\n",
    "train_acc['text'] = []\n",
    "train_acc['audio'] = []\n",
    "train_acc['video'] = []\n",
    "\n",
    "test_loss['text'] = []\n",
    "test_loss['audio'] = []\n",
    "test_loss['video'] = []\n",
    "test_acc['text'] = []\n",
    "test_acc['audio'] = []\n",
    "test_acc['video'] = []\n",
    "\n",
    "unimodal_results = {}\n",
    "\n",
    "def train():\n",
    "    for mode in models:\n",
    "        model = Unimodel(input_size[mode])\n",
    "        train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "        if(use_gpu):\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adagrad(params = model.parameters(), lr = 0.01)\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        print('begin training for unimodel ' + mode)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for e, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                if use_gpu:\n",
    "                    input_x = Variable(input_x.cuda(), requires_grad=True)\n",
    "                else:\n",
    "                    input_x = Variable(input_x, requires_grad=True)\n",
    "                y = cast_y(sequence_len[0], y).long()\n",
    "                if use_gpu:\n",
    "                    y = Variable(y.cuda())\n",
    "                predict_y, inter1 = model(input_x, sequence_len) \n",
    "                loss = criterion(predict_y.view(-1,2), y.view(-1))\n",
    "                _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                correct += (predicted.data == y.view(-1).data).sum().int().data[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.data[0]\n",
    "                #print(y.view(-1).size(0))\n",
    "                total += y.view(-1).size(0)\n",
    "            if(epoch %1 == 0):\n",
    "                #print(correct.item(), total)\n",
    "                print(\"training epoch %d loss %f acc %g\" %(epoch, running_loss, 1.* correct.item()/total))\n",
    "                train_loss[mode].append(running_loss/total)\n",
    "                train_acc[mode].append(correct/total)\n",
    "                running_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                #save_model(model, optimizer, os.path.join(save_dir + 'train' + mode+  'epoch %f.ckpt' % (epoch)))  \n",
    "        # save data, results to pickle.\n",
    "            model.eval()\n",
    "            for e, data in enumerate(test_loader):\n",
    "                input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                if use_gpu:\n",
    "                    input_x = Variable(input_x.cuda(), requires_grad=False)\n",
    "                else:\n",
    "                    input_x = Variable(input_x, requires_grad=False)\n",
    "                y = cast_y(sequence_len[0], y).long()\n",
    "                if use_gpu:\n",
    "                    y = Variable(y.cuda())\n",
    "                predict_y, inter2 = model(input_x, sequence_len) \n",
    "                loss = criterion(predict_y.view(-1,2), y.view(-1))\n",
    "                _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                correct += (predicted.data == y.view(-1).data).sum().int().data[0]\n",
    "                running_loss += loss.data[0]\n",
    "                total += y.view(-1).size(0)\n",
    "            if(epoch %1 == 0):\n",
    "                print(\"testing epoch %d loss %f acc %g\" %(epoch, running_loss, 1.* correct.item()/total))\n",
    "                test_loss[mode].append(running_loss)\n",
    "                test_acc[mode].append(1.* correct.item()/total)\n",
    "                running_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "        save_model(model, optimizer, os.path.join(save_dir + 'train ' + mode+  ' epoch %f .ckpt' % (epoch)))  \n",
    "        # save data, results to pickle.\n",
    "        unimodal_activations[mode +'_train'] = inter1\n",
    "    with open(save_dir + 'unimodal.pickle', rb) as handle:\n",
    "        pickle.dump(unimodal_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training for unimodel text\n",
      "training epoch 0 loss 2.062445 acc 0.889831\n",
      "testing epoch 0 loss 2.154599 acc 0.889045\n",
      "training epoch 1 loss 1.092797 acc 0.985165\n",
      "testing epoch 1 loss 2.697682 acc 0.871366\n",
      "training epoch 2 loss 0.880209 acc 0.98908\n",
      "testing epoch 2 loss 2.522632 acc 0.878359\n",
      "training epoch 3 loss 0.859958 acc 0.985165\n",
      "testing epoch 3 loss 2.819290 acc 0.874821\n",
      "training epoch 4 loss 0.794760 acc 0.988268\n",
      "testing epoch 4 loss 1.983819 acc 0.87257\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cs291gbn@cs291gbn-30904:~/Multimodal-Sentiment/result/traintextepoch 4.000000.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-fd3932939203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-8fbe52a4fe3d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m+\u001b[0m  \u001b[0;34m'epoch %f.ckpt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;31m# save data, results to pickle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0munimodal_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-8fbe52a4fe3d>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(net, optim, ckpt_fname)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             'optimizer': optim},                                                                                                                                                                                     \n\u001b[0;32m---> 43\u001b[0;31m             ckpt_fname)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cs291gbn@cs291gbn-30904:~/Multimodal-Sentiment/result/traintextepoch 4.000000.ckpt'"
     ]
    }
   ],
   "source": [
    "#train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.4)\n",
    "        self.fc1 = nn.Linear(hidden_size,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "        self.output = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        hidden_vect = (\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)),\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        output, hidden_vect = self.lstm(Variable(inputs), hidden_vect)\n",
    "        output = self.dropout2(self.tanh(output)) # apply drop out\n",
    "        output = self.dropout2(self.tanh(self.fc1(output))) # 100\n",
    "        output = self.softmax(self.fc2(output)) # 2\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
