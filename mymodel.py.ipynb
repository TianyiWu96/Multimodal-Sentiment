{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(1337)\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch, torch.nn as nn, math, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "import torch.utils.data\n",
    "Batch_size = 10\n",
    "Epochs = 200\n",
    "validation_split = 0.2\n",
    "shuffle = True\n",
    "\n",
    "def data_load(model, batch_size = 10, valid_size = 0.2):\n",
    "    # train, validation, test data \n",
    "    # question 1, if the train data and test data are from the same data distribution, will this be problem that the model overfit to the dataset\n",
    "    \n",
    "    with open('./input/'+model+'.pickle', 'rb') as handle:\n",
    "                (train_x, train_y, test_x, test_y, maxlen, train_len, test_len) = pickle.load(handle)\n",
    "    train_len = np.array(train_len)\n",
    "    test_len = np.array(test_len)\n",
    "    \n",
    "    train_x = torch.from_numpy(train_x).double()\n",
    "    train_y = torch.from_numpy(train_y).double()\n",
    "    test_x = torch.from_numpy(test_x).double()\n",
    "    test_y = torch.from_numpy(test_y).double()\n",
    "    train_len = torch.from_numpy(train_len)\n",
    "    test_len = torch.from_numpy(test_len)\n",
    "    \n",
    "    num_train = len(train_x) \n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    random_seed = 2018\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(train_x, train_y, train_len)\n",
    "    test = torch.utils.data.TensorDataset(test_x, test_y, test_len)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=valid_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size= batch_size, shuffle= True)\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc1 = nn.Linear(600,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "       \n",
    "    def forward(self, inputs, sequence_len): # input is 10 * 63 * 100\n",
    "        # training details:\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, sequence_len, batch_first=True)\n",
    "        hidden = (\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True),\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True))\n",
    "        #print(pack)\n",
    "        out, hidden = self.lstm(pack, hidden)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #print(unpacked.size())\n",
    "        output = self.dropout2(self.tanh(unpacked)) # apply drop out\n",
    "        output = self.dropout2(self.tanh(self.fc1(output))) # 100\n",
    "        output = self.fc2(output) # 2\n",
    "        #print(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training unimodel, use the index to trace the loss function.\n",
    "epochs = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "models = ['text', 'audio', 'video']\n",
    "input_size = {'text': 100, 'audio': 73, 'video':100}\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "def sorted_sequence(input_x,sequences,y):\n",
    "    # return the sorted sequence based on input_x\n",
    "    diction = {}\n",
    "    for i in range(int(sequences.size(0))):\n",
    "        diction[i] = sequences[i]\n",
    "    new_sequence = []\n",
    "    new_x = torch.zeros(input_x.size(0),input_x.size(1),input_x.size(2))\n",
    "    new_y = torch.zeros(y.size(0),y.size(1))\n",
    "    count = 0\n",
    "    for i, value in sorted(diction.items(), key=lambda x:x[1], reverse = True):\n",
    "        new_sequence.append(int(value))\n",
    "        new_x[count] = input_x[i]\n",
    "        new_y[count] = y[i]\n",
    "        count +=1\n",
    "    return new_x,new_sequence,new_y\n",
    "\n",
    "def cast_y(max_len, y):\n",
    "    new_y = torch.zeros(y.size(0), max_len)\n",
    "    for i in range(y.size(0)):\n",
    "        new_y[i] = y[i][:max_len]\n",
    "    return new_y\n",
    "\n",
    "def one_hot(train_y):\n",
    "    maxlabel = 1\n",
    "    new_y = torch.zeros((train_y.size(0),train_y.size(1), maxlabel+1))\n",
    "    for i in range(int(train_y.size(0))):\n",
    "        for j in range(int(train_y.size(1))):\n",
    "            new_y[i,j,int(train_y[i,j])] =1\n",
    "    return new_y\n",
    "loss_list = []\n",
    "for mode in models:\n",
    "    model = Unimodel(input_size[mode])\n",
    "    optimizer = optim.Adagrad(params = model.parameters(), lr = 0.01)\n",
    "    running_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "        batch = 0\n",
    "        for e, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "            input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "            input_x = Variable(input_x.cuda(), requires_grad=True)\n",
    "            y = cast_y(sequence_len[0], y)\n",
    "            y = Variable(y.view(-1).cuda())\n",
    "            predict_y = model(input_x, sequence_len)\n",
    "            predict_y = Variable(predict_y.view(-1,2),requires_grad=True)\n",
    "            #print(predict_y.view(-1,2).size(), y.view(-1,2).size())\n",
    "            loss = criterion(predict_y, y.long())\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss\n",
    "            batch += 1\n",
    "        #if(epoch %10 == 0):\n",
    "        print(\"epoch %d loss % f\" %(epoch, running_loss/batch))\n",
    "        loss_list.append(running_loss/batch)\n",
    "        running_loss = 0\n",
    "        batch = 0\n",
    "    save_model(model, optimizer, os.path.join(save_dir, '%03d.ckpt' % epoch))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.4)\n",
    "        self.fc1 = nn.Linear(hidden_size,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "        self.output = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs): \n",
    "        hidden_vect = (\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)),\n",
    "        Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        output, hidden_vect = self.lstm(Variable(inputs), hidden_vect)\n",
    "        output = self.dropout2(self.tanh(output)) # apply drop out\n",
    "        output = self.dropout2(self.tanh(self.fc1(output))) # 100\n",
    "        output = self.softmax(self.fc2(output)) # 2\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
