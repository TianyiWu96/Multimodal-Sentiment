{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(1337)\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch, torch.nn as nn, math, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size = 10\n",
    "# Epochs = 200\n",
    "# validation_split = 0.2\n",
    "shuffle = True\n",
    "\n",
    "def data_load(model, batch_size = 10, valid_size = 0.2):\n",
    "    # train, validation, test data \n",
    "    # question 1, if the train data and test data are from the same data distribution, will this be problem that the model overfit to the dataset\n",
    "    \n",
    "    with open('./input/'+model+'.pickle', 'rb') as handle:\n",
    "                (train_x, train_y, test_x, test_y, maxlen, train_len, test_len) = pickle.load(handle)\n",
    "    train_len = np.array(train_len)\n",
    "    test_len = np.array(test_len)\n",
    "    \n",
    "    train_x = torch.from_numpy(train_x).double()\n",
    "    train_y = torch.from_numpy(train_y).double()\n",
    "    test_x = torch.from_numpy(test_x).double()\n",
    "    test_y = torch.from_numpy(test_y).double()\n",
    "    train_len = torch.from_numpy(train_len)\n",
    "    test_len = torch.from_numpy(test_len)\n",
    "    \n",
    "    num_train = len(train_x) \n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    random_seed = 2018\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(train_x, train_y, train_len)\n",
    "    test = torch.utils.data.TensorDataset(test_x, test_y, test_len)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=train_sampler, num_workers=2)\n",
    "    valid_loader = torch.utils.data.DataLoader(train, batch_size= batch_size, sampler=valid_sampler, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size= batch_size, shuffle= True, num_workers=2)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unimodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 300, out_size = 100):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc1 = nn.Linear(600,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "       \n",
    "    def forward(self, inputs, sequence_len): # input is 10 * 63 * 100\n",
    "        # training details:\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, sequence_len, batch_first=True)\n",
    "        hidden = (\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True),\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True))\n",
    "        #print(pack)\n",
    "        out, hidden = self.lstm(pack, hidden)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #print(unpacked.size())\n",
    "        output = self.dropout2(self.tanh(unpacked)) # apply drop out\n",
    "        inter1 = self.dropout2(self.relu(self.fc1(output))) # 100\n",
    "        output = self.fc2(inter1) # 2\n",
    "        #print(output)\n",
    "        return output, inter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_sequence(input_x,sequences,y):\n",
    "    # return the sorted sequence based on input_x\n",
    "    diction = {}\n",
    "    for i in range(int(sequences.size(0))):\n",
    "        diction[i] = sequences[i]\n",
    "    new_sequence = []\n",
    "    new_x = torch.zeros(input_x.size(0),input_x.size(1),input_x.size(2))\n",
    "    new_y = torch.zeros(y.size(0),y.size(1))\n",
    "    count = 0\n",
    "    for i, value in sorted(diction.items(), key=lambda x:x[1], reverse = True):\n",
    "        new_sequence.append(int(value))\n",
    "        new_x[count] = input_x[i]\n",
    "        new_y[count] = y[i]\n",
    "        count +=1\n",
    "    return new_x,new_sequence,new_y\n",
    "\n",
    "def cast_y(max_len, y):\n",
    "    new_y = torch.zeros(y.size(0), max_len)\n",
    "    for i in range(y.size(0)):\n",
    "        new_y[i] = y[i][:max_len]\n",
    "    return new_y\n",
    "\n",
    "def save_model(net, optim, epoch, ckpt_fname):  \n",
    "        state_dict = net.state_dict()                                                                                                                                                                         \n",
    "        for key in state_dict.keys():                                                                                                                                                                                \n",
    "            state_dict[key] = state_dict[key].cpu()                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        torch.save({                                                                                                                                                                                                 \n",
    "            'epoch': epoch,                                                                                                                                                                                     \n",
    "            'state_dict': state_dict,                                                                                                                                                                                \n",
    "            'optimizer': optim},                                                                                                                                                                                     \n",
    "            ckpt_fname)\n",
    "        \n",
    "def one_hot(train_y):\n",
    "    maxlabel = 1\n",
    "    new_y = torch.zeros((train_y.size(0),train_y.size(1), maxlabel+1))\n",
    "    for i in range(int(train_y.size(0))):\n",
    "        for j in range(int(train_y.size(1))):\n",
    "            new_y[i,j,int(train_y[i,j])] = 1 \n",
    "    return new_y\n",
    "def generate_weight(sequence_length, y):\n",
    "    mask = torch.zeros((y.size(0),y.size(1)))\n",
    "    for i in range(y.size(0)):\n",
    "        for j in range(sequence_length[i]):\n",
    "            mask[i,j] = 1.0 \n",
    "    return mask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "epochs = 30\n",
    "models = ['audio', 'video','text']\n",
    "input_size = {'text': 100, 'audio': 73, 'video':100}\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "use_gpu = False\n",
    "print(use_gpu)\n",
    "unimodal_activations = {}\n",
    "\n",
    "train_loss = defaultdict(list)\n",
    "train_acc = defaultdict(list)\n",
    "test_loss = defaultdict(list)\n",
    "test_acc = defaultdict(list)\n",
    "valid_acc = defaultdict(list)\n",
    "valid_loss = defaultdict(list)\n",
    "\n",
    "unimodal_activations = {}\n",
    "criterion = nn.CrossEntropyLoss(size_average = False)\n",
    "\n",
    "def train():\n",
    "    for mode in models:\n",
    "        unimodal_activations = {}\n",
    "        model = Unimodel(input_size[mode])\n",
    "        #train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "        if(use_gpu):\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adagrad(params = model.parameters(), lr = 0.01)\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        print('begin training for unimodel ' + mode)\n",
    "        for epoch in range(epochs):\n",
    "            train_loader, valid_loader, test_loader = data_load(mode, 10, 0.2)\n",
    "            model.train()\n",
    "            for e, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                if use_gpu:\n",
    "                    input_x = Variable(input_x.cuda(), requires_grad=True)\n",
    "                else:\n",
    "                    input_x = Variable(input_x, requires_grad=True)\n",
    "                y = cast_y(sequence_len[0], y).long()\n",
    "                if use_gpu:\n",
    "                    y = Variable(y.cuda())\n",
    "                predict_y, inter1 = model(input_x, sequence_len)\n",
    "\n",
    "                loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                train_mask = generate_weight(sequence_len, y)\n",
    "                predicted = train_mask.view(-1).long() * predicted\n",
    "                total += sum(sequence_len)\n",
    "                correct += (predicted.data == y.view(-1).data).sum().int().data[0] - y.view(-1).size(0) + sum(sequence_len)\n",
    "                running_loss += loss.data[0]\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss[mode].append( 1. * running_loss)\n",
    "            train_acc[mode].append(1.* (correct.item()) /total)\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            for e, data in enumerate(valid_loader):\n",
    "                    input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                    input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                    if use_gpu:\n",
    "                        input_x = Variable(input_x.cuda(), requires_grad=False)\n",
    "                    else:\n",
    "                        input_x = Variable(input_x, requires_grad=False)\n",
    "                    y = cast_y(sequence_len[0], y).long()\n",
    "                    if use_gpu:\n",
    "                        y = Variable(y.cuda())\n",
    "                    predict_y, inter2 = model(input_x, sequence_len) \n",
    "                    loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                    running_loss += loss.data[0]\n",
    "                    _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "    \n",
    "                    total += sum(sequence_len)\n",
    "                    train_mask = generate_weight(sequence_len, y)\n",
    "                    predicted = train_mask.view(-1).long() * predicted\n",
    "                    correct += (predicted.data == y.view(-1).data).sum().int().data[0] - y.view(-1).size(0) + sum(sequence_len)\n",
    "                    \n",
    "            if(epoch %1 == 0):\n",
    "                    print(\"epoch %d train acc %g valid acc % f\" %(epoch, train_acc[mode][-1], 1.* correct.item()/total))\n",
    "                    valid_loss[mode].append(1.*running_loss)\n",
    "                    valid_acc[mode].append(1.* correct.item() /total)\n",
    "                    running_loss = 0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "            for e, data in enumerate(test_loader):\n",
    "                    input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "                    input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "                    if use_gpu:\n",
    "                        input_x = Variable(input_x.cuda(), requires_grad=False)\n",
    "                    else:\n",
    "                        input_x = Variable(input_x, requires_grad=False)\n",
    "                    y = cast_y(sequence_len[0], y).long()\n",
    "                    if use_gpu:\n",
    "                        y = Variable(y.cuda())\n",
    "                    predict_y, inter2 = model(input_x, sequence_len) \n",
    "                    loss = criterion(predict_y.view(-1,2), y.view(-1))/sum(sequence_len)\n",
    "                    _, predicted = torch.max(predict_y.view(-1,2).data, 1)\n",
    "                    train_mask = generate_weight(sequence_len, y)\n",
    "                    predicted = train_mask.view(-1).long() * predicted\n",
    "                    total += sum(sequence_len)\n",
    "                    correct += (predicted.data == y.view(-1).data).sum().int().data[0] - y.view(-1).size(0) + sum(sequence_len)\n",
    "                    running_loss += loss.data[0]\n",
    "                \n",
    "            if epoch % 5 == 0:\n",
    "                    print(\"test loss %f acc %g\" %(running_loss, 1.* correct.item()/total))\n",
    "            test_loss[mode].append(1.*running_loss)\n",
    "            test_acc[mode].append(1.* correct.item()/total)\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if(epoch > 10 and valid_acc[mode][-1] <= valid_acc[mode][-2] and valid_acc[mode][-2] <= valid_acc[mode][-3]):\n",
    "                print('stop early at %d' % epoch)\n",
    "                break\n",
    "        filename = 'result/train_' + mode+ '_epoch_%d.dat' % (epoch +1)\n",
    "        with open(filename,'w'):\n",
    "            save_model(model, optimizer, epoch, filename)  \n",
    "            \n",
    "        train_loader, valid_loader, test_loader = data_load(mode, input_size[mode], 0)\n",
    "        unimodal_activations = {}\n",
    "        for i,data in enumerate(train_loader):\n",
    "            input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "            input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "            input_x = Variable(input_x)\n",
    "            y = cast_y(sequence_len[0], y).long()\n",
    "            predict_y, inter1 = model(input_x, sequence_len)\n",
    "            unimodal_activations['train'] = inter1\n",
    "            unimodal_activations['train_len'] = sequence_len\n",
    "            unimodal_activations['train_y'] = y\n",
    "        for i,data in enumerate(test_loader):\n",
    "            input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "            input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "            input_x = Variable(input_x)\n",
    "            y = cast_y(sequence_len[0], y).long()\n",
    "            predict_y, inter1 = model(input_x, sequence_len)\n",
    "            unimodal_activations['test'] = inter1\n",
    "            unimodal_activations['test_len'] = sequence_len\n",
    "            unimodal_activations['test_y'] = y\n",
    "        \n",
    "        with open('result/' + mode + '_result_epoch_%d.pickle' %(epoch +1), 'wb') as handle:\n",
    "            result = defaultdict()\n",
    "            result['train_loss'] = train_loss\n",
    "            result['train_acc'] = train_acc\n",
    "            result['test_loss'] = test_loss\n",
    "            result['test_acc'] = test_acc\n",
    "            result['valid_acc'] = valid_acc\n",
    "            result['valid_loss'] = valid_loss\n",
    "            pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('result/' + mode + '_unimodel_epoch_%d'.pickle %(epoch +1), 'wb') as handle:\n",
    "            pickle.dump(unimodal_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training for unimodel audio\n",
      "epoch 0 train acc 0.486603 valid acc  0.427586\n",
      "test loss 4.370063 acc 0.378989\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c304f7985e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# text 0.783245 audio 0.378989\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-3f895410756b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0munimodal_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_result_epoch_%d'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'pickle'"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "train() # text 0.783245 audio 0.378989 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([62, 55, 100])\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = data_load('txt', 100, 0)\n",
    "# model = Unimodel(input_size['video'])\n",
    "for i,data in enumerate(train_loader):\n",
    "    input_x, y, sequence_len = data[0], data[1], data[2]\n",
    "    input_x, sequence_len, y = sorted_sequence(input_x,sequence_len, y)\n",
    "    input_x = Variable(input_x)\n",
    "    y = cast_y(sequence_len[0], y).long()\n",
    "    predict_y, inter1 = model(input_x, sequence_len)\n",
    "    print(i)\n",
    "    print(inter1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "print(unimodal_activations['text_train'].size())\n",
    "def multiModel(nn.Module()):\n",
    "     def __init__(self, input_size, hidden_size = 300, out_size = 500):\n",
    "        super(Unimodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1,batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(p = 0.6)\n",
    "        self.fc1 = nn.Linear(600,out_size) \n",
    "        self.dropout2 = nn.Dropout(p = 0.9)\n",
    "        self.tanh = nn.Hardtanh(-1,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(out_size, 2) \n",
    "       \n",
    "    def forward(self, inputs, sequence_len): # input is 10 * 63 * 100\n",
    "        # training details:\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, sequence_len, batch_first=True)\n",
    "        hidden = (\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True),\n",
    "        Variable(torch.zeros(2, inputs.size(0), self.hidden_size),requires_grad=True))\n",
    "        #print(pack)\n",
    "        out, hidden = self.lstm(pack, hidden)\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #print(unpacked.size())\n",
    "        output = self.dropout2(self.tanh(unpacked)) # apply drop out\n",
    "        inter1 = self.dropout2(self.relu(self.fc1(output))) # 100\n",
    "        output = self.fc2(inter1) # 2\n",
    "        #print(output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
